# LLM setup

Install:

```bash
ssh developer_username@192.168.1.XXX
mkdir ollama && cd ollama
curl -fsSL https://ollama.com/install.sh | sh
```

However, it is always better to follow the instructions on the project website.

### functionality check

Simple command line tests:

```bash
ollama --help
ollama list
ollama ps
ollama help pull
ollama help run
```

### choose a model

Pull and run a model:

```bash
ollama run llava:7b
```

## configure Ollama server

```bash
su -
systemctl status ollama --no-pager
nano /etc/systemd/system/ollama.service
```

Restart service:

```bash
systemctl daemon-reload
systemctl restart ollama
systemctl status ollama --no-pager
```

## make the service accessible remotely

Optional step, only if you want to make the service accessible remotely:

```bash
nano /etc/systemd/system/ollama.service
```

add the host line after the service entry, as shown below:

```conf
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

To then restart the service:

```bash
systemctl daemon-reload
systemctl restart ollama
systemctl status ollama --no-pager
```

Now it's time to open the tcp port `11434`:

```bash
systemctl status firewalld
firewall-cmd --permanent --zone=public --add-rich-rule 'rule family="ipv4" source address="192.168.1.0/24" port port=11434 protocol="tcp" accept'
firewall-cmd --reload
```
